{"cells":[{"cell_type":"code","execution_count":null,"id":"5bf4a837-31df-4340-ab92-3a49cc9b3c77","metadata":{"id":"5bf4a837-31df-4340-ab92-3a49cc9b3c77","outputId":"d3891619-0869-488a-eafc-d2687747c966"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found existing installation: torch 2.9.1+cpu\n","Uninstalling torch-2.9.1+cpu:\n","  Successfully uninstalled torch-2.9.1+cpu\n","Found existing installation: torchvision 0.24.1+cpu\n","Uninstalling torchvision-0.24.1+cpu:\n","  Successfully uninstalled torchvision-0.24.1+cpu\n","Found existing installation: torchaudio 2.9.1+cpu\n","Uninstalling torchaudio-2.9.1+cpu:\n","  Successfully uninstalled torchaudio-2.9.1+cpu\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip uninstall torch torchvision torchaudio -y"]},{"cell_type":"code","execution_count":null,"id":"f07c5bbd-7502-4d8e-b21d-d083d7e6539f","metadata":{"id":"f07c5bbd-7502-4d8e-b21d-d083d7e6539f","outputId":"4220e36f-c1fd-4fd9-8180-e2b4eb7015e2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Files removed: 759 (418.0 MB)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip cache purge"]},{"cell_type":"code","execution_count":null,"id":"de8f12ed-119a-42ea-a029-73deac49e699","metadata":{"id":"de8f12ed-119a-42ea-a029-73deac49e699","outputId":"b698e394-75ba-41d6-bc41-a4d669a47880"},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Looking in indexes: https://download.pytorch.org/whl/cu121\n","Collecting torch\n","  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp310-cp310-linux_x86_64.whl (780.4 MB)\n","\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m780.4/780.4 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m  \u001b[33m0:00:14\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n","\u001b[?25hCollecting torchvision\n","  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp310-cp310-linux_x86_64.whl (7.3 MB)\n","\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchaudio\n","  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /home/FajarInaya/.local/lib/python3.10/site-packages (from torch) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /home/FajarInaya/.local/lib/python3.10/site-packages (from torch) (4.15.0)\n","Requirement already satisfied: networkx in /home/FajarInaya/.local/lib/python3.10/site-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /home/FajarInaya/.local/lib/python3.10/site-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /home/FajarInaya/.local/lib/python3.10/site-packages (from torch) (2025.10.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m  \u001b[33m0:00:12\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5 (from torch)\n","  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n","\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Collecting triton==3.1.0 (from torch)\n","  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n","\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n","\u001b[?25hCollecting sympy==1.13.1 (from torch)\n","  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Downloading https://download.pytorch.org/whl/nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/FajarInaya/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: numpy in /home/FajarInaya/.local/lib/python3.10/site-packages (from torchvision) (2.2.6)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/FajarInaya/.local/lib/python3.10/site-packages (from torchvision) (12.0.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->torch) (2.0.1)\n","Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n","\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hDownloading https://download.pytorch.org/whl/nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n","\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n","\u001b[?25h\u001b[33mWARNING: Error parsing dependencies of distro-info: Invalid version: '1.1build1'\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Error parsing dependencies of python-debian: Invalid version: '0.1.43ubuntu1'\u001b[0m\u001b[33m\n","\u001b[0mInstalling collected packages: triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n","\u001b[2K  Attempting uninstall: sympyâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 0/17\u001b[0m [triton]\n","\u001b[2K    Found existing installation: sympy 1.14.0â”â”â”â”â”\u001b[0m \u001b[32m 0/17\u001b[0m [triton]\n","\u001b[2K    Uninstalling sympy-1.14.0:8;5;237mâ•º\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 1/17\u001b[0m [sympy]\n","\u001b[2K      Successfully uninstalled sympy-1.14.0;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 1/17\u001b[0m [sympy]\n","\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17/17\u001b[0m [torchaudio]37mâ”â”\u001b[0m \u001b[32m16/17\u001b[0m [torchaudio]lver-cu12]2]\n","\u001b[1A\u001b[2KSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 sympy-1.13.1 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121 triton-3.1.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"]},{"cell_type":"code","execution_count":null,"id":"6cd4b2d1-23d9-4ce0-8767-d9e3ac5812fc","metadata":{"id":"6cd4b2d1-23d9-4ce0-8767-d9e3ac5812fc","outputId":"022437a7-065f-4666-a3f2-8e26bede4d12"},"outputs":[{"name":"stdout","output_type":"stream","text":["PyTorch version: 2.5.1+cu121\n","CUDA available: True\n","GPU name: NVIDIA RTX A6000\n","CUDA version used by PyTorch: 12.1\n"]}],"source":["import torch\n","\n","print(f\"PyTorch version: {torch.__version__}\")\n","print(f\"CUDA available: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n","    print(f\"CUDA version used by PyTorch: {torch.version.cuda}\")\n","else:\n","    print(\"GPU not detected â€” check drivers or restart the kernel.\")"]},{"cell_type":"code","execution_count":null,"id":"9ef68f44-a749-41c6-a28a-318a2a3e95d5","metadata":{"id":"9ef68f44-a749-41c6-a28a-318a2a3e95d5","outputId":"93bd1b66-6c75-4664-a267-a74c5a885401"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/FajarInaya/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"name":"stdout","output_type":"stream","text":["CUDA available: True\n","Using device: cuda\n","GPU name: NVIDIA RTX A6000\n"]},{"name":"stderr","output_type":"stream","text":["Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14342/14342 [00:01<00:00, 7754.57 examples/s]\n","Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14342/14342 [00:05<00:00, 2746.78 examples/s]\n","Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/tmp/ipykernel_200700/1951711029.py:174: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='7175' max='7175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [7175/7175 10:43, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.136700</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.099200</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.096500</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.077000</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.078100</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.072500</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.063300</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.063800</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.049200</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.046900</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.046000</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.045200</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>0.032900</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>0.038100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Training complete. Model saved at /home/FajarInaya/Downloads/saved folders atttempt 1\n"]}],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Urdu Toxic Span Detection - GPU version (RTX A6000)\n","\"\"\"\n","\n","# ===============================\n","# 0. Imports\n","# ===============================\n","import os\n","from ast import literal_eval\n","import numpy as np\n","import pandas as pd\n","\n","# Remove any GPU-hiding environment variables\n","# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # REMOVED\n","os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # Hide TensorFlow warnings (if any)\n","\n","import torch\n","from datasets import Dataset\n","import evaluate\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForTokenClassification,\n","    TrainingArguments,\n","    Trainer,\n","    DataCollatorForTokenClassification\n",")\n","\n","# ===============================\n","# Device setup - Use GPU if available\n","# ===============================\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"CUDA available: {torch.cuda.is_available()}\")\n","print(f\"Using device: {DEVICE}\")\n","if torch.cuda.is_available():\n","    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n","\n","# ===============================\n","# 1. Config & Paths\n","# ===============================\n","DATA_CSV = r\"/home/FajarInaya/Downloads/UrduToxicSpanDataset_Attempt1.csv\"\n","SAVE_DIR = r\"/home/FajarInaya/Downloads/saved folders atttempt 1\"\n","MODEL_NAME = \"xlm-roberta-base\"\n","MAX_LENGTH = 128\n","TRAIN_BATCH_SIZE = 8  # You can increase to 16 or 32 on A6000 if VRAM allows\n","EVAL_BATCH_SIZE = 8\n","NUM_EPOCHS = 5\n","LEARNING_RATE = 3e-5\n","\n","# ===============================\n","# 2. Load dataset\n","# ===============================\n","df = pd.read_csv(DATA_CSV)\n","\n","def ensure_list(example):\n","    if isinstance(example[\"tokens\"], str):\n","        example[\"tokens\"] = literal_eval(example[\"tokens\"])\n","    if isinstance(example[\"BIO_tags\"], str):\n","        example[\"BIO_tags\"] = literal_eval(example[\"BIO_tags\"])\n","    return example\n","\n","dataset = Dataset.from_pandas(df)\n","dataset = dataset.map(ensure_list)\n","\n","# ===============================\n","# 3. Label mappings\n","# ===============================\n","label_list = [\"O\", \"B-Toxic\", \"I-Toxic\"]\n","label2id = {label: i for i, label in enumerate(label_list)}\n","id2label = {i: label for label, i in label2id.items()}\n","\n","# ===============================\n","# 4. Tokenizer\n","# ===============================\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","\n","# ===============================\n","# 5. Tokenize and align labels\n","# ===============================\n","def tokenize_and_align_labels(example):\n","    tokens = example[\"tokens\"]\n","    labels = example[\"BIO_tags\"]\n","    tokenized_input = tokenizer(\n","        tokens,\n","        is_split_into_words=True,\n","        truncation=True,\n","        padding=\"max_length\",\n","        max_length=MAX_LENGTH\n","    )\n","    word_ids = tokenized_input.word_ids()\n","    label_ids = []\n","    previous_word_idx = None\n","    for word_idx in word_ids:\n","        if word_idx is None:\n","            label_ids.append(-100)\n","        elif word_idx == previous_word_idx:\n","            label_ids.append(-100)  # Subword gets -100\n","        else:\n","            label_ids.append(label2id[labels[word_idx]])\n","        previous_word_idx = word_idx\n","    tokenized_input[\"labels\"] = label_ids\n","    return tokenized_input\n","\n","tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=False)\n","tokenized_dataset = tokenized_dataset.remove_columns([\"tokens\", \"BIO_tags\"])\n","\n","# ===============================\n","# 6. Split dataset\n","# ===============================\n","dataset_split = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n","train_dataset = dataset_split[\"train\"]\n","eval_dataset = dataset_split[\"test\"]\n","\n","train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","eval_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","\n","# ===============================\n","# 7. Load model\n","# ===============================\n","model = AutoModelForTokenClassification.from_pretrained(\n","    MODEL_NAME,\n","    num_labels=len(label_list),\n","    id2label=id2label,\n","    label2id=label2id\n",")\n","model.to(DEVICE)  # Moves model to GPU\n","\n","# ===============================\n","# 8. Training arguments (GPU enabled)\n","# ===============================\n","training_args = TrainingArguments(\n","    output_dir=\"./toxic_model_results\",\n","    do_eval=True,\n","    learning_rate=LEARNING_RATE,\n","    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n","    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n","    num_train_epochs=NUM_EPOCHS,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    save_total_limit=2,\n","    report_to=\"none\",\n","    # no_cuda=True  # REMOVED - this was forcing CPU!\n",")\n","\n","# ===============================\n","# 9. Data collator & metrics\n","# ===============================\n","data_collator = DataCollatorForTokenClassification(tokenizer)\n","\n","seqeval = evaluate.load(\"seqeval\")\n","\n","def compute_metrics(pred):\n","    predictions, labels = pred\n","    predictions = np.argmax(predictions, axis=2)\n","    true_labels = [\n","        [id2label[l] for l in label if l != -100]\n","        for label in labels\n","    ]\n","    true_predictions = [\n","        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n","    return {\n","        \"precision\": results[\"overall_precision\"],\n","        \"recall\": results[\"overall_recall\"],\n","        \"f1\": results[\"overall_f1\"],\n","        \"accuracy\": results[\"overall_accuracy\"],\n","    }\n","\n","# ===============================\n","# 10. Trainer\n","# ===============================\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics\n",")\n","\n","# ===============================\n","# 11. Train model\n","# ===============================\n","trainer.train()\n","\n","# ===============================\n","# 12. Save model & tokenizer\n","# ===============================\n","trainer.save_model(SAVE_DIR)\n","tokenizer.save_pretrained(SAVE_DIR)\n","print(f\"Training complete. Model saved at {SAVE_DIR}\")"]},{"cell_type":"code","execution_count":null,"id":"f03e4541-e7de-46e9-b3b9-739505ac3658","metadata":{"id":"f03e4541-e7de-46e9-b3b9-739505ac3658","outputId":"b3f4a8fd-0ce2-44ab-9df1-3d6888881a56"},"outputs":[{"name":"stdout","output_type":"stream","text":["CUDA available: True\n","Using device: cuda\n","GPU name: NVIDIA RTX A6000\n"]},{"name":"stderr","output_type":"stream","text":["The tokenizer you are loading from '/home/FajarInaya/Downloads/saved folders atttempt 1' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"]},{"name":"stdout","output_type":"stream","text":["âœ… Model loaded successfully on GPU\n","\n","ğŸ“Š Evaluating model on test set...\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='359' max='359' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [359/359 00:05]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","ğŸ¯ Evaluation Results:\n"," eval_loss: 0.1183\n"," eval_precision: 0.5961\n"," eval_recall: 0.6074\n"," eval_f1: 0.6017\n"," eval_accuracy: 0.9741\n"," eval_runtime: 5.7833\n"," eval_samples_per_second: 496.0830\n"," eval_steps_per_second: 62.0750\n"," epoch: 5.0000\n","\n","ğŸ§ª Urdu Sample Testing:\n","\n","Ù…Ø«Ø§Ù„ 1: ØªÙ… Ø¨ÛØª Ø§Ø­Ù…Ù‚ ÛÙˆ Ø§ÙˆØ± Ú©ÙˆØ¦ÛŒ ØªÙ…ÛÛŒÚº Ù¾Ø³Ù†Ø¯ Ù†ÛÛŒÚº Ú©Ø±ØªØ§\n"]},{"ename":"AttributeError","evalue":"'dict' object has no attribute 'word_ids'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 126\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_sentences, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mÙ…Ø«Ø§Ù„ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 126\u001b[0m     spans \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_toxic_spans\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spans:\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m âš ï¸ Ø²ÛØ±ÛŒÙ„Û’ Ø§Ù„ÙØ§Ø¸:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[3], line 67\u001b[0m, in \u001b[0;36mpredict_toxic_spans\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     64\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoding)\n\u001b[1;32m     65\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs\u001b[38;5;241m.\u001b[39mlogits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 67\u001b[0m word_ids \u001b[38;5;241m=\u001b[39m \u001b[43mencoding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_ids\u001b[49m(batch_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Updated for newer Transformers versions\u001b[39;00m\n\u001b[1;32m     68\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m [model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mid2label[p] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m predictions]\n\u001b[1;32m     70\u001b[0m toxic_spans \u001b[38;5;241m=\u001b[39m []\n","\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'word_ids'"]}],"source":["# ===============================\n","# âœ… Step 7: Urdu Toxic Span Detection\n","# Evaluation + Inference\n","# (GPU-ENABLED â€“ aligned with training on RTX A6000)\n","# ===============================\n","import torch\n","import numpy as np\n","import json\n","from collections import Counter\n","from transformers import AutoTokenizer, AutoModelForTokenClassification\n","\n","# -------------------------------\n","# 1. Device (Use GPU â€“ consistent with training)\n","# -------------------------------\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"CUDA available: {torch.cuda.is_available()}\")\n","print(f\"Using device: {DEVICE}\")\n","if torch.cuda.is_available():\n","    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n","\n","# -------------------------------\n","# 2. Load fine-tuned model\n","# -------------------------------\n","model_path = SAVE_DIR  # SAME path used during training\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","model = AutoModelForTokenClassification.from_pretrained(model_path)\n","model.to(DEVICE)\n","model.eval()\n","print(\"âœ… Model loaded successfully on GPU\")\n","\n","# -------------------------------\n","# 3. Evaluation on test set (Trainer-based)\n","# -------------------------------\n","# Note: trainer is from the training script and already has the model on GPU\n","print(\"\\nğŸ“Š Evaluating model on test set...\")\n","eval_results = trainer.evaluate()\n","print(\"\\nğŸ¯ Evaluation Results:\")\n","for key, value in eval_results.items():\n","    if isinstance(value, float):\n","        print(f\" {key}: {value:.4f}\")\n","    else:\n","        print(f\" {key}: {value}\")\n","\n","# -------------------------------\n","# 4. Inference function (WORD-LEVEL FIXED + GPU)\n","# -------------------------------\n","def predict_toxic_spans(text):\n","    \"\"\"\n","    Predict toxic spans in Urdu text using BIO tagging.\n","    Returns list of toxic spans.\n","    \"\"\"\n","    words = text.strip().split()\n","    encoding = tokenizer(\n","        words,\n","        is_split_into_words=True,\n","        return_tensors=\"pt\",\n","        truncation=True,\n","        max_length=128,\n","        padding=\"max_length\"\n","    )\n","    encoding = {k: v.to(DEVICE) for k, v in encoding.items()}\n","\n","    with torch.no_grad():\n","        outputs = model(**encoding)\n","        predictions = torch.argmax(outputs.logits, dim=2)[0].cpu().numpy()\n","\n","    word_ids = encoding.word_ids(batch_index=0)  # Updated for newer Transformers versions\n","    predicted_labels = [model.config.id2label[p] for p in predictions]\n","\n","    toxic_spans = []\n","    current_span = None\n","    previous_word_id = None\n","\n","    for label, word_id in zip(predicted_labels, word_ids):\n","        if word_id is None:\n","            continue\n","        if word_id == previous_word_id:\n","            continue  # Skip subwords (already handled by word_ids repeat)\n","\n","        word = words[word_id]\n","\n","        if label.startswith(\"B-\"):\n","            if current_span:\n","                toxic_spans.append(current_span)\n","            current_span = {\n","                \"label\": label[2:],\n","                \"words\": [word]\n","            }\n","        elif label.startswith(\"I-\") and current_span and current_span[\"label\"] == label[2:]:\n","            current_span[\"words\"].append(word)\n","        else:\n","            if current_span:\n","                toxic_spans.append(current_span)\n","                current_span = None\n","\n","        previous_word_id = word_id\n","\n","    if current_span:\n","        toxic_spans.append(current_span)\n","\n","    return [\n","        {\n","            \"text\": \" \".join(span[\"words\"]),\n","            \"label\": span[\"label\"]\n","        }\n","        for span in toxic_spans\n","    ]\n","\n","# -------------------------------\n","# 5. Test on Urdu example sentences\n","# -------------------------------\n","test_sentences = [\n","    \"ØªÙ… Ø¨ÛØª Ø§Ø­Ù…Ù‚ ÛÙˆ Ø§ÙˆØ± Ú©ÙˆØ¦ÛŒ ØªÙ…ÛÛŒÚº Ù¾Ø³Ù†Ø¯ Ù†ÛÛŒÚº Ú©Ø±ØªØ§\",\n","    \"ÛŒÛ Ø§ÛŒÚ© Ø¹Ø§Ù… Ø¬Ù…Ù„Û ÛÛ’ Ø¬Ø³ Ù…ÛŒÚº Ú©ÙˆØ¦ÛŒ Ø²ÛØ±ÛŒÙ„Ø§ Ù¾Ù† Ù†ÛÛŒÚº ÛÛ’\",\n","    \"Ú†Ù¾ Ú©Ø±Ùˆ ØªÙ… Ø¨ÛŒÙˆÙ‚ÙˆÙ ÛÙˆ\",\n","    \"Ù…ÛŒÚº Ø¢Ù¾ Ú©ÛŒ Ø±Ø§Ø¦Û’ Ø³Û’ Ø§Ø­ØªØ±Ø§Ù… Ú©Û’ Ø³Ø§ØªÚ¾ Ø§Ø®ØªÙ„Ø§Ù Ú©Ø±ØªØ§ ÛÙˆÚº\",\n","    \"Ø§Ù¾Ù†Û’ Ø¢Ù¾ Ú©Ùˆ Ù…Ø§Ø± ÚˆØ§Ù„Ùˆ ÛØ§Ø±Û’ ÛÙˆØ¦Û’\",\n","    \"ØªÙ… ÙˆØ§Ù‚Ø¹ÛŒ Ù…ÛŒÚº Ú©Ù…ÛŒÙ†Û’ ÛÙˆ\",\n","    \"Ø¢Ù¾ Ú©Ø§ Ø¯Ù† Ø§Ú†Ú¾Ø§ Ú¯Ø²Ø±Û’\",\n","    \"Ú©Ù†Ø¬Ø± Ú©ÛŒ Ø§ÙˆÙ„Ø§Ø¯ ØªÙ…ÛØ§Ø±ÛŒ Ù…Ø§Úº Ú©Ùˆ Ø¨Ú¾ÛŒ Ø´Ø±Ù… Ù†ÛÛŒÚº Ø¢ØªÛŒ\",\n","]\n","\n","print(\"\\nğŸ§ª Urdu Sample Testing:\\n\")\n","for i, sentence in enumerate(test_sentences, 1):\n","    print(f\"Ù…Ø«Ø§Ù„ {i}: {sentence}\")\n","    spans = predict_toxic_spans(sentence)\n","    if spans:\n","        print(\" âš ï¸ Ø²ÛØ±ÛŒÙ„Û’ Ø§Ù„ÙØ§Ø¸:\")\n","        for span in spans:\n","            print(f\" - '{span['text']}' [{span['label']}]\")\n","    else:\n","        print(\" âœ… Ú©ÙˆØ¦ÛŒ Ø²ÛØ±ÛŒÙ„Ø§ Ù¾Ù† Ù†ÛÛŒÚº Ù…Ù„Ø§\")\n","    print()\n","\n","# -------------------------------\n","# 6. Save predictions on test set\n","# -------------------------------\n","print(\"\\nğŸ’¾ Saving test-set predictions...\")\n","test_predictions = []\n","\n","# Note: eval_dataset.indices gives original dataframe indices\n","for idx in range(len(eval_dataset)):\n","    original_idx = eval_dataset.indices[idx]\n","    original_text = df.iloc[original_idx][\"text\"]  # Assuming your original df has a \"text\" column\n","    spans = predict_toxic_spans(original_text)\n","    test_predictions.append({\n","        \"id\": original_idx,  # Using original index for traceability\n","        \"text\": original_text,\n","        \"toxic_spans\": spans,\n","        \"num_toxic_spans\": len(spans)\n","    })\n","\n","with open(\"urdu_test_predictions.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump(test_predictions, f, indent=2, ensure_ascii=False)\n","\n","print(\"âœ… Predictions saved to urdu_test_predictions.json\")\n","\n","# -------------------------------\n","# 7. Summary statistics\n","# -------------------------------\n","total_texts = len(test_predictions)\n","texts_with_toxicity = sum(p[\"num_toxic_spans\"] > 0 for p in test_predictions)\n","total_toxic_spans = sum(p[\"num_toxic_spans\"] for p in test_predictions)\n","\n","print(\"\\nğŸ“ˆ Summary Statistics:\")\n","print(f\" Total texts analyzed: {total_texts}\")\n","print(f\" Texts with toxicity: {texts_with_toxicity} ({texts_with_toxicity/total_texts*100:.1f}%)\")\n","print(f\" Total toxic spans found: {total_toxic_spans}\")\n","print(f\" Avg toxic spans per text: {total_toxic_spans/total_texts:.2f}\")\n","\n","# -------------------------------\n","# 8. Label distribution\n","# -------------------------------\n","all_labels = [span[\"label\"] for pred in test_predictions for span in pred[\"toxic_spans\"]]\n","label_counts = Counter(all_labels)\n","\n","print(\"\\nğŸ·ï¸ Label Distribution:\")\n","if label_counts:\n","    for label, count in label_counts.most_common():\n","        print(f\" {label}: {count}\")\n","else:\n","    print(\" No toxic spans detected in test set.\")\n","\n","print(\"\\nâœ… Evaluation & Inference Complete!\")"]},{"cell_type":"code","execution_count":null,"id":"93b78115-4489-4fe9-8e8a-3e0f38eee38f","metadata":{"id":"93b78115-4489-4fe9-8e8a-3e0f38eee38f"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}