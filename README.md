ðŸ” Multimodal Urdu Toxic Span Detection
A multimodal framework for detecting toxic content in Urdu text and audio, combining XLM-RoBERTa for text-based toxic span detection with Wav2Vec2 for audio-based toxicity classification via late fusion.

ðŸ“Œ Overview
This project extends MUTEX (Multilingual Transformer + CRF for Urdu Toxic Span Detection) by adding an audio modality, creating the first multimodal pipeline for Urdu toxic content detection.
| Modality | Model | F1 Score |
|---|---|---|
| Text only | XLM-RoBERTa | 67% |
| Audio only | Wav2Vec2 | 70% |
| **Multimodal Fusion** | **XLM-RoBERTa + Wav2Vec2** | **79.34%** |

ðŸ—‚ï¸ Project Structure
ðŸ“ Project
â”‚
â”œâ”€â”€ ðŸ““ Notebook 1 â€” Text Model (MUTEX)
â”‚   â””â”€â”€ XLM-RoBERTa fine-tuned on URTOX for toxic span detection
â”‚
â”œâ”€â”€ ðŸ““ Notebook 2 â€” Audio Model
â”‚   â”œâ”€â”€ TTS audio generation from URTOX using Edge TTS
â”‚   â”œâ”€â”€ Wav2Vec2 feature extraction
â”‚   â””â”€â”€ Audio toxic classifier training
â”‚
â”œâ”€â”€ ðŸ““ Notebook 3 â€” Fusion
â”‚   â”œâ”€â”€ Late fusion (0.6 text + 0.4 audio)
â”‚   â”œâ”€â”€ Final multimodal evaluation
â”‚   â””â”€â”€ Real-world WhatsApp audio testing pipeline
â”‚
â””â”€â”€ ðŸ“ Dataset
    â”œâ”€â”€ URTOX_v2.csv                    â† Original text dataset (14,342 samples)
    â”œâ”€â”€ urdu_toxic_audio_dataset.csv    â† Dataset with audio paths
    â””â”€â”€ urdu_toxic_audio_og/            â† MP3 audio files folder

ðŸ“Š Dataset â€” URTOX
URTOX is a manually annotated Urdu toxic span dataset containing 14,342 samples collected from:

| Source | Samples | Toxic % | Non-Toxic % |
|---|---|---|---|
| Social Media (X, Instagram, Reddit) | 5,254 | 57% | 43% |
| Urdu Newspapers | 4,300 | 52% | 48% |
| YouTube | 4,788 | 56% | 44% |
| **Total** | **14,342** | **54%** | **46%** |

**Dataset Columns
id            â†’ unique identifier
text          â†’ raw Urdu text
tokens        â†’ tokenized words (list)
BIO_tags      â†’ token-level BIO annotation (B-Toxic, I-Toxic, O)
toxic_spans   â†’ toxic word spans
toxic_list    â†’ list of toxic words
label         â†’ sentence-level label (toxic / non_toxic)
sub_label     â†’ toxicity category (hate, insult, offensive, neutral)
audio_path    â†’ path to corresponding MP3 file


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   INPUT: Audio File                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                         â”‚
          â–¼                         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   Wav2Vec2    â”‚        â”‚  XLM-RoBERTa    â”‚
  â”‚  (Audio       â”‚        â”‚  (Text Model)   â”‚
  â”‚   Features)   â”‚        â”‚                 â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                         â”‚
          â–¼                         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Audio Toxic   â”‚        â”‚  Toxic Span     â”‚
  â”‚ Classifier    â”‚        â”‚  Detection      â”‚
  â”‚ P(toxic)=70% â”‚        â”‚  P(toxic)=67%  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                         â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚    Late Fusion         â”‚
          â”‚  0.4 Ã— audio +         â”‚
          â”‚  0.6 Ã— text            â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚     FINAL OUTPUT       â”‚
          â”‚  âœ… Toxic / Non-Toxic  â”‚
          â”‚  âš ï¸  Toxic Spans       â”‚
          â”‚  ðŸ“Š Confidence Score   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          
## ðŸ“ About

This project focuses on toxic span detection in Urdu text using a 
transformer-based token classification approach. The goal is not only 
to identify whether a sentence contains toxic content, but also to 
**locate and highlight the specific toxic words or spans** within the text.

**No prior work exists on Urdu toxic span detection** â€” this is the 
first proposed framework for fine-grained toxicity localization in Urdu.

> ðŸ“„ A journal paper has been submitted to **Elsevier Applied Soft Computing** 
> and will be published soon.

### ðŸŽ¯ Task Summary

| Component | Details |
|---|---|
| Task | Toxic span detection at word/token level in Urdu |
| Model | XLM-RoBERTa fine-tuned for token classification |
| Dataset | Newly created and manually annotated Urdu toxic span dataset |
| Output | Highlighted toxic words/spans within each input sentence |
| Use Case | Moderation, abusive language detection, explainable toxicity analysis |
| Framework | HuggingFace Transformers + PyTorch |
